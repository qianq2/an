继续上次，本将模型进行一点优化，首先是数据处理模块将，将数据进行了分字处理构建了字符表，除去了一些不常用的标点符号，将数据集进行9：1划分为训练集和验证集。
把LSTM模型改为了两层，训练损失下降得比较明显，结果导致过拟合非常严重，也有可能之前也很严重只是我没有划分验证集，又改回了单层模型，也加入了Dropout层、混合精度训练和使用了l2正则来降低模型的复杂程度和增强泛化能力，又加入早停法来解决训练效率问题。
根据上次gpu没跑满的情况针对加载数据的进行多线程有优化，参数调整等。模型的训练有了明显提升之前跑30多个epoch需要2个多小时，修改后只需要半个小时，验证集的收敛比之前好很多，但是长文本生成的效果还是不佳，仍存在情节逻辑断裂问题。分析生成样本发
现，LSTM对长文本的依捉能力明显不行，这指向了传统循环神经网络的结构性局限。由于数据较少，下一步计划尝试更换预训练模型gpt2或bert，再加入一些数据集，以突破现有生成质量的瓶颈。

<img width="655" alt="925d23ec30737ce228d85e975a0c292" src="https://github.com/user-attachments/assets/2b23445f-4830-44ba-a231-0fd2c3e6f1fa" />

<img width="1199" alt="e59be014b88335e3ac90b2f410cbaff" src="https://github.com/user-attachments/assets/c3cdcf6b-00b5-4dad-90f4-4fd3dbda716d" />

本次修改基本将上次的项目进行全面的修改，除了基本框架没动之外，基本都改过了，但是还有一些优化没尝试，还有一些模型指标也没尝试如BLUE，困惑度等。在修改的过程也遇到一些七七八八的问题如模型的输入输出不匹配，多线程不兼容需要封装等问题，也学到很多东西。
